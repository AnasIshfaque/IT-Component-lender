{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch import tensor\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms,models\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import Accuracy, Precision, Recall, F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0.485,0.456,0.406])\n",
    "std = np.array([0.229,0.224,0.225])\n",
    "\n",
    "data_transforms = {\n",
    "    'train':transforms.Compose([\n",
    "        transforms.Resize(299),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ]),\n",
    "    'val':transforms.Compose([\n",
    "        transforms.Resize(299),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'train_dataset'\n",
    "sets = ['train','val']\n",
    "image_datasets = {x:datasets.ImageFolder(os.path.join(data_dir,x),\n",
    "                                         data_transforms[x]) \n",
    "                  for x in ['train','val']}\n",
    "dataloaders = {x:torch.utils.data.DataLoader(image_datasets[x],batch_size=4,\n",
    "                                             shuffle=True,num_workers=0)\n",
    "                for x in ['train','val']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "dataset_sizes = {x:len(image_datasets[x]) for x in ['train','val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # initialize metric\n",
    "    metric = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=3)\n",
    "    predicted_labels = []\n",
    "    ground_truth_labels = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs-1}')\n",
    "        print('-'*10)\n",
    "        \n",
    "        #Training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            #Iterate over data\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                #forward\n",
    "                #track history only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _,preds = torch.max(outputs,1)\n",
    "                    loss = criterion(outputs,labels)\n",
    "                    \n",
    "                    #backward + optimize only in train\n",
    "                    if phase == 'train':\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                #statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            predicted_labels.append(preds.cpu())\n",
    "            ground_truth_labels.append(labels.cpu())\n",
    "\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            #deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "    \n",
    "    #calculate accuracy\n",
    "    predicted_labels = torch.cat(predicted_labels)\n",
    "    ground_truth_labels = torch.cat(ground_truth_labels)\n",
    "    accuracy = Accuracy(task=\"multiclass\", num_classes=3)\n",
    "    accuracy(predicted_labels, ground_truth_labels)\n",
    "    print(f'Accuracy: {accuracy.compute():.4f}')\n",
    "    \n",
    "    #calculate precision\n",
    "    precision = Precision(task=\"multiclass\", average='macro', num_classes=3)\n",
    "    precision(predicted_labels, ground_truth_labels)\n",
    "    print(f'Precision: {precision.compute():.4f}')\n",
    "    \n",
    "    #calculate recall\n",
    "    recall = Recall(task=\"multiclass\", average='macro', num_classes=3)\n",
    "    recall(predicted_labels, ground_truth_labels)\n",
    "    print(f'Recall: {recall.compute():.4f}')\n",
    "    \n",
    "    #calculate f1 score\n",
    "    f1 = F1Score(task=\"multiclass\", average='macro', num_classes=3)\n",
    "    f1(predicted_labels, ground_truth_labels)\n",
    "    print(f'F1: {f1.compute():.4f}')\n",
    "    \n",
    "    #calculate confusion matrix\n",
    "    cm = torchmetrics.functional.confusion_matrix(predicted_labels, ground_truth_labels, num_classes=3, task=\"multiclass\")\n",
    "    print(f'Confusion Matrix: \\n{cm}')    \n",
    "    #load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anas/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/anas/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (3 x 3). Kernel size: (5 x 5). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/media/anas/9EEE4A8AEE4A5B23/ISHFAQUE/UIU BSCSE/TinyPaper 2024/TransferLearningPractice/IT-Component-lender/Transfer_Learning/tfr_nn_inception_v3.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/anas/9EEE4A8AEE4A5B23/ISHFAQUE/UIU%20BSCSE/TinyPaper%202024/TransferLearningPractice/IT-Component-lender/Transfer_Learning/tfr_nn_inception_v3.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#scheduler\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/anas/9EEE4A8AEE4A5B23/ISHFAQUE/UIU%20BSCSE/TinyPaper%202024/TransferLearningPractice/IT-Component-lender/Transfer_Learning/tfr_nn_inception_v3.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m step_lr_scheduler \u001b[39m=\u001b[39m lr_scheduler\u001b[39m.\u001b[39mStepLR(optimizer,step_size\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m,gamma\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/anas/9EEE4A8AEE4A5B23/ISHFAQUE/UIU%20BSCSE/TinyPaper%202024/TransferLearningPractice/IT-Component-lender/Transfer_Learning/tfr_nn_inception_v3.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m model_ft \u001b[39m=\u001b[39m train_model(pretrained_model,criterion,optimizer,step_lr_scheduler,num_epochs\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m)\n",
      "\u001b[1;32m/media/anas/9EEE4A8AEE4A5B23/ISHFAQUE/UIU BSCSE/TinyPaper 2024/TransferLearningPractice/IT-Component-lender/Transfer_Learning/tfr_nn_inception_v3.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/anas/9EEE4A8AEE4A5B23/ISHFAQUE/UIU%20BSCSE/TinyPaper%202024/TransferLearningPractice/IT-Component-lender/Transfer_Learning/tfr_nn_inception_v3.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m#forward\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/anas/9EEE4A8AEE4A5B23/ISHFAQUE/UIU%20BSCSE/TinyPaper%202024/TransferLearningPractice/IT-Component-lender/Transfer_Learning/tfr_nn_inception_v3.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m#track history only in train\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/anas/9EEE4A8AEE4A5B23/ISHFAQUE/UIU%20BSCSE/TinyPaper%202024/TransferLearningPractice/IT-Component-lender/Transfer_Learning/tfr_nn_inception_v3.ipynb#W6sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/anas/9EEE4A8AEE4A5B23/ISHFAQUE/UIU%20BSCSE/TinyPaper%202024/TransferLearningPractice/IT-Component-lender/Transfer_Learning/tfr_nn_inception_v3.ipynb#W6sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/anas/9EEE4A8AEE4A5B23/ISHFAQUE/UIU%20BSCSE/TinyPaper%202024/TransferLearningPractice/IT-Component-lender/Transfer_Learning/tfr_nn_inception_v3.ipynb#W6sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     _,preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs,\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/anas/9EEE4A8AEE4A5B23/ISHFAQUE/UIU%20BSCSE/TinyPaper%202024/TransferLearningPractice/IT-Component-lender/Transfer_Learning/tfr_nn_inception_v3.ipynb#W6sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs,labels)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/inception.py:166\u001b[0m, in \u001b[0;36mInception3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m InceptionOutputs:\n\u001b[1;32m    165\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform_input(x)\n\u001b[0;32m--> 166\u001b[0m     x, aux \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(x)\n\u001b[1;32m    167\u001b[0m     aux_defined \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maux_logits\n\u001b[1;32m    168\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/inception.py:138\u001b[0m, in \u001b[0;36mInception3._forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mAuxLogits \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m--> 138\u001b[0m         aux \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mAuxLogits(x)\n\u001b[1;32m    139\u001b[0m \u001b[39m# N x 768 x 17 x 17\u001b[39;00m\n\u001b[1;32m    140\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMixed_7a(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/inception.py:386\u001b[0m, in \u001b[0;36mInceptionAux.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    384\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv0(x)\n\u001b[1;32m    385\u001b[0m \u001b[39m# N x 128 x 5 x 5\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m    387\u001b[0m \u001b[39m# N x 768 x 1 x 1\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[39m# Adaptive average pooling\u001b[39;00m\n\u001b[1;32m    389\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39madaptive_avg_pool2d(x, (\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/inception.py:405\u001b[0m, in \u001b[0;36mBasicConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 405\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)\n\u001b[1;32m    406\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn(x)\n\u001b[1;32m    407\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mrelu(x, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (3 x 3). Kernel size: (5 x 5). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "pretrained_model = models.inception_v3(pretrained=True)\n",
    "\n",
    "#exchange the last layer\n",
    "num_ftrs = pretrained_model.fc.in_features\n",
    "pretrained_model.fc = nn.Linear(num_ftrs, 3)\n",
    "pretrained_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(pretrained_model.parameters(),lr=0.001)\n",
    "\n",
    "#scheduler\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer,step_size=7,gamma=0.1)\n",
    "\n",
    "model_ft = train_model(pretrained_model,criterion,optimizer,step_lr_scheduler,num_epochs=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 1.1424 Acc: 0.4000\n",
      "val Loss: 1.0305 Acc: 0.4000\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 1.1012 Acc: 0.4000\n",
      "val Loss: 1.0350 Acc: 0.3333\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 1.0612 Acc: 0.4444\n",
      "val Loss: 0.9528 Acc: 0.6000\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 1.0133 Acc: 0.4667\n",
      "val Loss: 0.8885 Acc: 0.6667\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.9009 Acc: 0.6222\n",
      "val Loss: 0.8523 Acc: 0.7333\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.8667 Acc: 0.6889\n",
      "val Loss: 0.8046 Acc: 0.6667\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.8729 Acc: 0.6444\n",
      "val Loss: 0.6754 Acc: 0.9333\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.8294 Acc: 0.7111\n",
      "val Loss: 0.6870 Acc: 0.9333\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.9360 Acc: 0.6000\n",
      "val Loss: 0.6868 Acc: 0.8667\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.8655 Acc: 0.6444\n",
      "val Loss: 0.6565 Acc: 0.9333\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.8863 Acc: 0.6889\n",
      "val Loss: 0.6965 Acc: 0.9333\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.8933 Acc: 0.6222\n",
      "val Loss: 0.6802 Acc: 0.9333\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.8392 Acc: 0.7111\n",
      "val Loss: 0.6403 Acc: 1.0000\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.8214 Acc: 0.6667\n",
      "val Loss: 0.6759 Acc: 0.8667\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.8020 Acc: 0.7778\n",
      "val Loss: 0.6871 Acc: 0.9333\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.8210 Acc: 0.7111\n",
      "val Loss: 0.6779 Acc: 0.9333\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.8412 Acc: 0.6889\n",
      "val Loss: 0.6563 Acc: 0.9333\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.7938 Acc: 0.7556\n",
      "val Loss: 0.7046 Acc: 0.9333\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.8018 Acc: 0.7556\n",
      "val Loss: 0.6836 Acc: 0.8667\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.7775 Acc: 0.7778\n",
      "val Loss: 0.6893 Acc: 0.9333\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.8627 Acc: 0.6667\n",
      "val Loss: 0.6477 Acc: 0.9333\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.8150 Acc: 0.7556\n",
      "val Loss: 0.6855 Acc: 0.9333\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.7616 Acc: 0.8222\n",
      "val Loss: 0.7210 Acc: 0.9333\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.7574 Acc: 0.9111\n",
      "val Loss: 0.6927 Acc: 0.9333\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.8264 Acc: 0.8000\n",
      "val Loss: 0.6535 Acc: 0.9333\n",
      "\n",
      "Training complete in 0m 10s\n",
      "Best val Acc: 1.000000\n",
      "Accuracy: 0.7300\n",
      "Precision: 0.7320\n",
      "Recall: 0.7259\n",
      "F1: 0.7285\n",
      "Confusion Matrix: \n",
      "tensor([[25,  7,  2],\n",
      "        [ 6, 30,  4],\n",
      "        [ 4,  4, 18]])\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = models.inception_v3(pretrained=True)\n",
    "\n",
    "for param in pretrained_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "#exchange the last layer\n",
    "num_ftrs = pretrained_model.fc.in_features\n",
    "pretrained_model.fc = nn.Linear(num_ftrs, 3)\n",
    "pretrained_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(pretrained_model.parameters(),lr=0.001)\n",
    "\n",
    "#scheduler\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer,step_size=7,gamma=0.1)\n",
    "\n",
    "model_ft = train_model(pretrained_model,criterion,optimizer,step_lr_scheduler,num_epochs=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vspyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
